{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이름/학번\n",
    "\n",
    "이름:\n",
    "\n",
    "학번:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example dataset\n",
    "\n",
    "강의를 위해서 임의의 dataset을 준비하겠습니다.\n",
    "예제로 봐주시고, 큰 물리적 의미는 부여하지 않겠습니다.\n",
    "\n",
    "- Data는 장미과와 국화과의 A 효소, B 효소, C 효소, D 효소를 측정한 값이라고 가정합니다.\n",
    "- Label은 각 sample이 장미인지 (0) 국화인지 (1)에 대한 정보라고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_feature = 4\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_batch = torch.randn(batch_size, num_feature)\n",
    "Y_batch = (torch.sum(X_batch, dim=1)>0).type(torch.float).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation 정리\n",
    "\n",
    "강의자료와 비교하면 \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{X_batch} = \n",
    "\\begin{bmatrix}\n",
    "(x^{(1)})^\\top\\\\\n",
    "(x^{(2)})^\\top\\\\\n",
    "\\vdots \\\\\n",
    "(x^{(m)})^\\top\n",
    "\\end{bmatrix}, \\quad\n",
    "\\text{Y_batch} = \n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots \\\\\n",
    "y^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Problem: Single Neuron\n",
    "\n",
    "- 한개의 neuron이 있다고 가정하고 $\\mathbb{R}^4$ 를 입력받아서 $\\mathbb{R}$로 출력한다고 가정합니다.\n",
    "- Activation 함수는 ReLU 함수, 즉, \n",
    "\\begin{align*}\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "\\end{align*}\n",
    "를 사용한다고 가정합니다.\n",
    "\n",
    "Neuron을 통해서 batch sample 전체를\n",
    "\\begin{align*}\n",
    "Z = \\begin{bmatrix}\n",
    "(w^T x^{(1)} + b)^T \\\\\n",
    "(w^T x^{(2)} + b)^T \\\\\n",
    "\\vdots \\\\\n",
    "(w^T x^{(m)} + b)^T\n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "연산을 수행해서 $Z$를 구하세요.\n",
    "\n",
    "- $w$는 random Gaussian으로 생성하세요. 위에서 예기한 입력과 출력이 맞도록 weight를 생성하세요.\n",
    "- Bias $b$는 1로 설정\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "W = torch.randn(num_feature, 1)\n",
    "b = 1\n",
    "Z = torch.matmul(X_batch, W)+b\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 연산을 하는지 확인하도록 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_loop = torch.empty(batch_size, 1)\n",
    "for i in torch.arange(batch_size):\n",
    "    z_loop[i,:] = torch.matmul(W.T, X_batch[i,:].T)+b\n",
    "print(z_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "ReLU 함수를 작성하고 위에서 찾은 $Z$의 각 원소에 ReLU 함수를 적용하여 `a`라는 변수에 저장하세요.\n",
    "\n",
    "- torch.clamp() 함수를 공부하고 적용하세요\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성:\n",
    "\n",
    "def ReLU(x):\n",
    "    return True\n",
    "\n",
    "A = ReLU(Z)\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Network\n",
    "\n",
    "- 한개의 Layer에 $k=5$개의 Neuron 이 있는 network를 구성하고 출력을 구하세요\n",
    "- Activation function은 모든 neuron에 ReLU를 적용합니다\n",
    "- 모든 weight는 Gaussian 분포로 랜덤 생성하세요 `torch.randn()`\n",
    "- $i$ 번째 neuron의 weight들을 $w_i$라고 할때,\n",
    "\\begin{align*}\n",
    "\\text{W} = \\begin{bmatrix}\n",
    "w_1, w_2, w_3, w_4, w_5\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "라고 하고, weight matrix `W`를 만드세요.\n",
    "  - `W = torch.randn(???, ???)` 으로 생성\n",
    "- Bias 역시 `b`라는 `tensor`에 저장하고, 각 neuron 별로 `1`로 설정합니다\n",
    "  - `b = torch.ones(???,???)`\n",
    "- 아래 problem 2-2에서 수업에서 배운 $Z$ 행렬과 $A$ 행렬을 구하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-1\n",
    "`Z` 행렬과 `A` 행렬의 차원은 어떻게 나와야하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답 작성)\n",
    "\n",
    "Z 는 (?,?), \n",
    "A 는 (?,?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2-2 \n",
    "위에서 요구한 코딩을 완성하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "# W = \n",
    "# b = \n",
    "# Z = \n",
    "# A = \n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: \n",
    "\n",
    "- $x^{(3)}$ 를 입력으로하는 2번째 Neuron의 결과값을 출력하세요\n",
    "- 위에서 구한 `A[i, j]`  인덱싱을 통해서 출력하세요\n",
    "- `Python`의 인덱싱은 `0`부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "# print('answer = ', A[?, ?])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Multi-Layer network\n",
    "\n",
    "- 3개의 layer가 있는 network를 구성합니다.\n",
    "- 2번째 layer의 입력 크기는 $k^{[1]}=16$, 출력 크기는 $k^{[2]}=6$\n",
    "- 마지막 layer의 출력은 $k^{[3]}=1$개의 neuron으로 구성\n",
    "- 각 layer의 연산값을 구하세요. \n",
    "  - 각 layer의 선형 변환 결과값은 `Z1`, `Z2`, `Z3`로 저장하세요\n",
    "  - 각 layer의 결과값은 `A1`, `A2`, `A3`로 저장하세요\n",
    "- 모든 weight는 Gaussian 랜덤 변수로 생성, bias는 1로 구성된 벡터로 생성함\n",
    "- 각 layer의 weight는 `W1`, `W2`, `W3`로하고, bias는 `b1`, `b2`, `b3`로 생성함\n",
    "- Activation 함수는 `ReLU`를 적용하세요\n",
    "- Loop 없이 행렬연산으로 구생하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "# W1= \n",
    "# W2= \n",
    "# W3= \n",
    "# b1= \n",
    "# b2= \n",
    "# b3= \n",
    "\n",
    "# Z1 = \n",
    "# Z2 = \n",
    "# Z3 = \n",
    "\n",
    "# A1 = \n",
    "# A2 = \n",
    "# A3 = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z1.shape)\n",
    "print(Z1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z2.shape)\n",
    "print(Z2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z3.shape)\n",
    "print(Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "- 위에서 구한 `A3[i,j]`의 인덱싱을 통해서 $h_\\theta(x^{(3)})$ 을 출력하세요\n",
    "- 역시 `python`인덱싱은 `0` 부터 시작한다는 것을 주의하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "\n",
    "# print('h_theta(x3) = ', A3[?,?])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6\n",
    "위에서 공부한 것을 함수로 만들어 보도록 하겠습니다.\n",
    "아래 한 layer의 선형 변환을 연산하는 class를 만들어 보도록 하죠.\n",
    "\n",
    "- Class는 `my_linear_layer()`\n",
    "  - `__init__(self, n_input, n_output)` 함수:\n",
    "    - `self.W` 변수 초기화: Weight 행렬 `self.W`를 램덤 Gaussian 생성 (차원에 맞는...)\n",
    "    - `self.b` 변수 초기화: bias 벡터 `self.b`를 모두 `1`인 벡터 생성 (차원에 맞는...)\n",
    "  - `forward(A)` 함수:\n",
    "    - 입력: `A`는 sample batch $X$ 또는 전 layer에서 들어오는 입력 batch $A^{[\\ell-1]}$을 입력하는 자리\n",
    "    - return 값\n",
    "      - `Z` 변수는 $A^{[\\ell-1]}$의 선형 변환 값, 즉 $Z^{[\\ell]}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답 작성\n",
    "class my_linear_layer():\n",
    "    def __init__(self, n_input, n_output):\n",
    "        #self.W = \n",
    "        #self.b = \n",
    "    \n",
    "    def forward(self,A):\n",
    "        #Z = \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "답을 확인하기 위해서 `n_input=num_feature`과 `n_output = 5` 인 `my_linear_layer` instance 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = my_linear_layer(num_feature, 5)\n",
    "mll.forward(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mll.W)\n",
    "print(mll.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Linear Layer with `torch.nn`\n",
    "위에서 수행한 작업을 `pytorch`에서는 `torch.nn.Linear`라는 명령어로 쉽게 구현할 수 있습니다.\n",
    "아래 예제를 보도록 하죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#W = torch.randn(num_feature, 5)\n",
    "L1 = nn.Linear(num_feature, 5)\n",
    "Zll = L1(X_batch)\n",
    "Zll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1.weight = nn.Parameter(W.T)\n",
    "L1.bias.data.fill_(1.0)\n",
    "Zll2 = L1(X_batch)\n",
    "Zll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
